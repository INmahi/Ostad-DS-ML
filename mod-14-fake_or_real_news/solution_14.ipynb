{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dac41996",
   "metadata": {},
   "source": [
    "# Fake vs Real News Classification using Naive Bayes\n",
    "## ishat noor mahi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be34f2a6",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f67fb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2c1242f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 129</th>\n",
       "      <th>Unnamed: 130</th>\n",
       "      <th>Unnamed: 131</th>\n",
       "      <th>Unnamed: 132</th>\n",
       "      <th>Unnamed: 133</th>\n",
       "      <th>Unnamed: 134</th>\n",
       "      <th>Unnamed: 135</th>\n",
       "      <th>Unnamed: 136</th>\n",
       "      <th>Unnamed: 137</th>\n",
       "      <th>Unnamed: 138</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text label Unnamed: 2  \\\n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE        NaN   \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE        NaN   \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL        NaN   \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE        NaN   \n",
       "4  It's primary day in New York and front-runners...  REAL        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7 Unnamed: 8  \\\n",
       "0        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "1        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "2        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "3        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "4        NaN        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "  Unnamed: 9  ... Unnamed: 129 Unnamed: 130 Unnamed: 131 Unnamed: 132  \\\n",
       "0        NaN  ...          NaN          NaN          NaN          NaN   \n",
       "1        NaN  ...          NaN          NaN          NaN          NaN   \n",
       "2        NaN  ...          NaN          NaN          NaN          NaN   \n",
       "3        NaN  ...          NaN          NaN          NaN          NaN   \n",
       "4        NaN  ...          NaN          NaN          NaN          NaN   \n",
       "\n",
       "  Unnamed: 133 Unnamed: 134 Unnamed: 135 Unnamed: 136 Unnamed: 137  \\\n",
       "0          NaN          NaN          NaN          NaN          NaN   \n",
       "1          NaN          NaN          NaN          NaN          NaN   \n",
       "2          NaN          NaN          NaN          NaN          NaN   \n",
       "3          NaN          NaN          NaN          NaN          NaN   \n",
       "4          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "  Unnamed: 138  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "\n",
       "[5 rows x 139 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('fake_or_real_news.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aea8eeb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7795 entries, 0 to 7794\n",
      "Columns: 139 entries, text to Unnamed: 138\n",
      "dtypes: object(139)\n",
      "memory usage: 8.3+ MB\n",
      "Missing Values:\n",
      "text             866\n",
      "label           1040\n",
      "Unnamed: 2      7477\n",
      "Unnamed: 3      7554\n",
      "Unnamed: 4      7616\n",
      "                ... \n",
      "Unnamed: 134    7794\n",
      "Unnamed: 135    7794\n",
      "Unnamed: 136    7794\n",
      "Unnamed: 137    7794\n",
      "Unnamed: 138    7794\n",
      "Length: 139, dtype: int64\n",
      "Duplicate Rows: 1142\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "print(\"Missing Values:\")\n",
    "print(df.isnull().sum())\n",
    "print(\"Duplicate Rows:\", df.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "467d5732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution:\n",
      "label\n",
      "REAL                                                                                                                                                                                                           3161\n",
      "FAKE                                                                                                                                                                                                           3154\n",
      " or naturalization or by jus sanguinis – inherited through ancestors/parents                                                                                                                                      3\n",
      " etc.                                                                                                                                                                                                             2\n",
      " Philippines                                                                                                                                                                                                      2\n",
      "                                                                                                                                                                                                               ... \n",
      " but we did not withdraw from the plutonium agreement                                                                                                                                                             1\n",
      " the United States made a unilateral announcement that they would not dilute this weapons-grade plutonium but would store it in some beds and so forth.                                                           1\n",
      " including those on Syria. Perhaps we will be able to come back to this. We are ready                                                                                                                             1\n",
      " points                                                                                                                                                                                                           1\n",
      " sweeping his arm from this ancient Christian village toward the horizon. The Iraqi captain was searching for tunnels dug by Islamic State fighters. The officer stomped on the ground. ‘Here. We found one       1\n",
      "Name: count, Length: 437, dtype: int64\n",
      "Class Percentages:\n",
      "label\n",
      "REAL                                                                                                                                                                                                           46.794967\n",
      "FAKE                                                                                                                                                                                                           46.691340\n",
      " or naturalization or by jus sanguinis – inherited through ancestors/parents                                                                                                                                    0.044412\n",
      " etc.                                                                                                                                                                                                           0.029608\n",
      " Philippines                                                                                                                                                                                                    0.029608\n",
      "                                                                                                                                                                                                                 ...    \n",
      " but we did not withdraw from the plutonium agreement                                                                                                                                                           0.014804\n",
      " the United States made a unilateral announcement that they would not dilute this weapons-grade plutonium but would store it in some beds and so forth.                                                         0.014804\n",
      " including those on Syria. Perhaps we will be able to come back to this. We are ready                                                                                                                           0.014804\n",
      " points                                                                                                                                                                                                         0.014804\n",
      " sweeping his arm from this ancient Christian village toward the horizon. The Iraqi captain was searching for tunnels dug by Islamic State fighters. The officer stomped on the ground. ‘Here. We found one     0.014804\n",
      "Name: proportion, Length: 437, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Class Distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(\"Class Percentages:\")\n",
    "print(df['label'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9363a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Statistics:\n",
      "        text_length   word_count\n",
      "count   6929.000000  6929.000000\n",
      "mean    4251.583345   700.783663\n",
      "std     4412.048384   731.276753\n",
      "min        1.000000     0.000000\n",
      "25%     1228.000000   203.000000\n",
      "50%     3247.000000   533.000000\n",
      "75%     5861.000000   970.000000\n",
      "max    32727.000000  5913.000000\n"
     ]
    }
   ],
   "source": [
    "# Check text statistics\n",
    "df['text_length'] = df['text'].str.len()\n",
    "df['word_count'] = df['text'].str.split().str.len()\n",
    "print(\"Text Statistics:\")\n",
    "print(df[['text_length', 'word_count']].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1c382",
   "metadata": {},
   "source": [
    "## Step 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8970e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "print(\"Handling Missing Values...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check for missing values in text column\n",
    "missing_text = df['text'].isnull().sum()\n",
    "print(f\"Missing values in 'text': {missing_text}\")\n",
    "\n",
    "# Drop rows with missing text if any\n",
    "if missing_text > 0:\n",
    "    df = df.dropna(subset=['text'])\n",
    "    print(f\"Dropped {missing_text} rows with missing text\")\n",
    "else:\n",
    "    print(\"No missing values in text column\")\n",
    "\n",
    "missing_labels = df['label'].isnull().sum()\n",
    "print(f\"Missing values in 'label': {missing_labels}\")\n",
    "\n",
    "if missing_labels > 0:\n",
    "    df = df.dropna(subset=['label'])\n",
    "    print(f\"Dropped {missing_labels} rows with missing labels\")\n",
    "else:\n",
    "    print(\"No missing values in label column\")\n",
    "\n",
    "print(\"\\nFinal dataset shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a17f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_before = df.duplicated().sum()\n",
    "if duplicates_before > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Removed {duplicates_before} duplicate rows\")\n",
    "    print(f\"Dataset shape after removing duplicates: {df.shape}\")\n",
    "else:\n",
    "    print(\"No duplicate rows found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dd671d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "REAL                                                                                                                                                                                                           3161\n",
      "FAKE                                                                                                                                                                                                           3154\n",
      " or naturalization or by jus sanguinis – inherited through ancestors/parents                                                                                                                                      3\n",
      " etc.                                                                                                                                                                                                             2\n",
      " Philippines                                                                                                                                                                                                      2\n",
      "                                                                                                                                                                                                               ... \n",
      " but we did not withdraw from the plutonium agreement                                                                                                                                                             1\n",
      " the United States made a unilateral announcement that they would not dilute this weapons-grade plutonium but would store it in some beds and so forth.                                                           1\n",
      " including those on Syria. Perhaps we will be able to come back to this. We are ready                                                                                                                             1\n",
      " points                                                                                                                                                                                                           1\n",
      " sweeping his arm from this ancient Christian village toward the horizon. The Iraqi captain was searching for tunnels dug by Islamic State fighters. The officer stomped on the ground. ‘Here. We found one       1\n",
      "Name: count, Length: 437, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare features (X) and target (y)\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "print(y.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d072cb07",
   "metadata": {},
   "source": [
    "## Step 4: Split Data into Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c09be6c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X_train, X_test, y_train, y_test = \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2940\u001b[39m, in \u001b[36mtrain_test_split\u001b[39m\u001b[34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[39m\n\u001b[32m   2936\u001b[39m         CVClass = ShuffleSplit\n\u001b[32m   2938\u001b[39m     cv = CVClass(test_size=n_test, train_size=n_train, random_state=random_state)\n\u001b[32m-> \u001b[39m\u001b[32m2940\u001b[39m     train, test = \u001b[38;5;28mnext\u001b[39m(\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstratify\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   2942\u001b[39m train, test = ensure_common_namespace_device(arrays[\u001b[32m0\u001b[39m], train, test)\n\u001b[32m   2944\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(\n\u001b[32m   2945\u001b[39m     chain.from_iterable(\n\u001b[32m   2946\u001b[39m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m arrays\n\u001b[32m   2947\u001b[39m     )\n\u001b[32m   2948\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2429\u001b[39m, in \u001b[36mStratifiedShuffleSplit.split\u001b[39m\u001b[34m(self, X, y, groups)\u001b[39m\n\u001b[32m   2424\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m groups \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2425\u001b[39m     warnings.warn(\n\u001b[32m   2426\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe groups parameter is ignored by \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   2427\u001b[39m         \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m   2428\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2429\u001b[39m y = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43my\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2430\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().split(X, y, groups)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:1105\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1099\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1100\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m{\u001b[39;00marray.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1101\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m while dim <= 2 is required\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1102\u001b[39m     )\n\u001b[32m   1104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1114\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\sklearn\\utils\\validation.py:105\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_array_api \u001b[38;5;129;01mand\u001b[39;00m X.dtype == np.dtype(\u001b[33m\"\u001b[39m\u001b[33mobject\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_nan:\n\u001b[32m    104\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _object_dtype_isnan(X).any():\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mInput contains NaN\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    107\u001b[39m \u001b[38;5;66;03m# We need only consider float arrays, hence can early return for all else.\u001b[39;00m\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m xp.isdtype(X.dtype, (\u001b[33m\"\u001b[39m\u001b[33mreal floating\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcomplex floating\u001b[39m\u001b[33m\"\u001b[39m)):\n",
      "\u001b[31mValueError\u001b[39m: Input contains NaN"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ccb0e",
   "metadata": {},
   "source": [
    "## Step 5: Text Vectorization (Feature Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7fa189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text to numerical features using TF-IDF\n",
    "print(\"Converting text to TF-IDF features...\")\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_features=5000,  # Limit to top 5000 features\n",
    "    stop_words='english',  # Remove common English stop words\n",
    "    max_df=0.7,  # Ignore terms that appear in more than 70% of documents\n",
    "    min_df=5  # Ignore terms that appear in fewer than 5 documents\n",
    ")\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "print(f\"Training features shape: {X_train_tfidf.shape}\")\n",
    "print(f\"Testing features shape: {X_test_tfidf.shape}\")\n",
    "print(f\"Vocabulary size: {len(tfidf_vectorizer.vocabulary_)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73a833e",
   "metadata": {},
   "source": [
    "## Step 6: Build Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac418203",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Naive Bayes Model...\")\n",
    "\n",
    "nb_model = MultinomialNB(alpha=1.0)  \n",
    "nb_model.fit(X_train_tfidf, y_train)\n",
    "\n",
    "print(\"Model training completed!\")\n",
    "print(f\"Model type: {type(nb_model).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3b7148",
   "metadata": {},
   "source": [
    "## Step 7: Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc0fc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on training and testing sets\n",
    "y_train_pred = nb_model.predict(X_train_tfidf)\n",
    "y_test_pred = nb_model.predict(X_test_tfidf)\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_test_pred_proba = nb_model.predict_proba(X_test_tfidf)\n",
    "\n",
    "print(\"Predictions completed!\")\n",
    "print(f\"Training predictions shape: {y_train_pred.shape}\")\n",
    "print(f\"Testing predictions shape: {y_test_pred.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc0b16",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ec5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"MODEL ACCURACY SCORES\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Training Accuracy: {train_accuracy:.4f} ({train_accuracy*100:.2f}%)\")\n",
    "print(f\"Testing Accuracy:  {test_accuracy:.4f} ({test_accuracy*100:.2f}%)\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acafe130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nCONFUSION MATRIX\")\n",
    "print(\"=\"*50)\n",
    "print(cm)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['FAKE', 'REAL'], \n",
    "            yticklabels=['FAKE', 'REAL'],\n",
    "            cbar_kws={'label': 'Count'})\n",
    "plt.title('Confusion Matrix - Naive Bayes Model', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('True Label', fontsize=12)\n",
    "plt.xlabel('Predicted Label', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2260a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate classification report\n",
    "print(\"\\nCLASSIFICATION REPORT\")\n",
    "print(\"=\"*50)\n",
    "print(classification_report(y_test, y_test_pred, target_names=['FAKE', 'REAL']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8b7ead",
   "metadata": {},
   "source": [
    "## Step 8: Model Interpretation - Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b256eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature log probabilities\n",
    "feature_names = np.array(tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Get top features for each class\n",
    "n_top_features = 20\n",
    "\n",
    "# For FAKE news (class 0)\n",
    "fake_log_prob = nb_model.feature_log_prob_[0]\n",
    "top_fake_indices = np.argsort(fake_log_prob)[-n_top_features:][::-1]\n",
    "top_fake_features = feature_names[top_fake_indices]\n",
    "top_fake_values = fake_log_prob[top_fake_indices]\n",
    "\n",
    "# For REAL news (class 1)\n",
    "real_log_prob = nb_model.feature_log_prob_[1]\n",
    "top_real_indices = np.argsort(real_log_prob)[-n_top_features:][::-1]\n",
    "top_real_features = feature_names[top_real_indices]\n",
    "top_real_values = real_log_prob[top_real_indices]\n",
    "\n",
    "print(\"TOP 20 FEATURES FOR FAKE NEWS\")\n",
    "print(\"=\"*50)\n",
    "for i, (feature, value) in enumerate(zip(top_fake_features, top_fake_values), 1):\n",
    "    print(f\"{i:2d}. {feature:20s} (log prob: {value:.4f})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TOP 20 FEATURES FOR REAL NEWS\")\n",
    "print(\"=\"*50)\n",
    "for i, (feature, value) in enumerate(zip(top_real_features, top_real_values), 1):\n",
    "    print(f\"{i:2d}. {feature:20s} (log prob: {value:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d46197",
   "metadata": {},
   "source": [
    "## Step 10: Test the Model with Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e748821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample articles from the test set\n",
    "sample_indices = np.random.choice(X_test.index, size=5, replace=False)\n",
    "\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for idx in sample_indices:\n",
    "    text = X_test.loc[idx]\n",
    "    true_label = y_test.loc[idx]\n",
    "    \n",
    "    # Transform and predict\n",
    "    text_tfidf = tfidf_vectorizer.transform([text])\n",
    "    prediction = nb_model.predict(text_tfidf)[0]\n",
    "    probability = nb_model.predict_proba(text_tfidf)[0]\n",
    "    \n",
    "    print(f\"\\nArticle Text (first 200 chars):\")\n",
    "    print(f\"{text[:200]}...\")\n",
    "    print(f\"\\nTrue Label: {true_label}\")\n",
    "    print(f\"Predicted Label: {prediction}\")\n",
    "    print(f\"Confidence: FAKE={probability[0]:.4f}, REAL={probability[1]:.4f}\")\n",
    "    print(f\"Result: {'✓ CORRECT' if prediction == true_label else '✗ INCORRECT'}\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfbeebf",
   "metadata": {},
   "source": [
    "## Step 11: Summary of Findings\n",
    "\n",
    "### Key Results:\n",
    "\n",
    "The Naive Bayes classification model has been successfully built and evaluated for fake vs real news detection.\n",
    "\n",
    "#### Model Performance:\n",
    "- **Algorithm Used**: Multinomial Naive Bayes\n",
    "- **Feature Extraction**: TF-IDF Vectorization with 5000 max features\n",
    "- **Train-Test Split**: 80-20 split with stratification\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
